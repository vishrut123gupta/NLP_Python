{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad2a0804-6a6d-453d-998f-ba052ee2d0cd",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\">DATA EXTRACTION AND NLP</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f6ba4f-57d2-40cc-b093-0fa285e66270",
   "metadata": {},
   "source": [
    "#### Problem Statement: Extracting Article Text and Headings from URLs provided and performing Text Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43acb8b0-80a4-487f-8aab-a56eb243f55f",
   "metadata": {},
   "source": [
    "***NOTE:** Multiple files have been provided as a part of the problem statement and are present alongwith this notebook. Kindly refer to the same for better understanding!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec9a2f18-92f8-4a06-8461-3ef4fa60fb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing relevant modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import os\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7d79f6-95f8-40b5-993c-72c13d2e25eb",
   "metadata": {},
   "source": [
    "Problem Input is given in the form of an excel file hence translating into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6276441-666d-420b-87a0-b1fe097373eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blackassign0001</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blackassign0002</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blackassign0003</td>\n",
       "      <td>https://insights.blackcoffer.com/internet-dema...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blackassign0004</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-cyber...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blackassign0005</td>\n",
       "      <td>https://insights.blackcoffer.com/ott-platform-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            URL_ID                                                URL\n",
       "0  blackassign0001  https://insights.blackcoffer.com/rising-it-cit...\n",
       "1  blackassign0002  https://insights.blackcoffer.com/rising-it-cit...\n",
       "2  blackassign0003  https://insights.blackcoffer.com/internet-dema...\n",
       "3  blackassign0004  https://insights.blackcoffer.com/rise-of-cyber...\n",
       "4  blackassign0005  https://insights.blackcoffer.com/ott-platform-..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_excel(\"./assign_ques_details/Input.xlsx\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96617a7b-f412-4640-b38c-963d5757ed44",
   "metadata": {},
   "source": [
    "##### Now, as we have to extract article text and headings from URLs, we’re using BeautifulSoup library to extract the article headings and text using the class name in a function as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2150758d-950f-4533-973d-075ac5cdee66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the text and heading of an article from url\n",
    "def get_article_text_and_heading(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status() #raising error message incase of response error\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        article_container = soup.find('div', class_='td-post-content')\n",
    "        if not article_container:\n",
    "            return 'No heading found', 'No content found' # handling no content\n",
    "\n",
    "        heading = soup.find('h1')\n",
    "        heading_text = heading.get_text(strip=True) if heading else 'No heading found' # finding article heading and removing whitespaces\n",
    "        \n",
    "        content_elements = article_container.find_all(['p', 'li']) # finding all article text using paragraph and list tags\n",
    "        article_text = ' '.join([element.get_text(strip=True) for element in content_elements]) \n",
    "        return heading_text, article_text \n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        return 'Error fetching page', str(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d559397-cdd1-499e-ae45-85edc749d2ac",
   "metadata": {},
   "source": [
    "##### We then iterate over the DataFrame’s rows containing the URLs and scrap and assign the article headings as well as text into new columns. This is done using the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fccbba9f-08f0-449d-bbea-804aa2a4d3d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>Heading</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blackassign0001</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "      <td>Rising IT cities and its impact on the economy...</td>\n",
       "      <td>We have seen a huge development and dependence...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blackassign0002</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "      <td>Rising IT Cities and Their Impact on the Econo...</td>\n",
       "      <td>Throughout history, from the industrial revolu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blackassign0003</td>\n",
       "      <td>https://insights.blackcoffer.com/internet-dema...</td>\n",
       "      <td>Internet Demand’s Evolution, Communication Imp...</td>\n",
       "      <td>Introduction In the span of just a few decades...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blackassign0004</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-cyber...</td>\n",
       "      <td>Rise of Cybercrime and its Effect in upcoming ...</td>\n",
       "      <td>The way we live, work, and communicate has unq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blackassign0005</td>\n",
       "      <td>https://insights.blackcoffer.com/ott-platform-...</td>\n",
       "      <td>OTT platform and its impact on the entertainme...</td>\n",
       "      <td>The year 2040 is poised to witness a continued...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            URL_ID                                                URL  \\\n",
       "0  blackassign0001  https://insights.blackcoffer.com/rising-it-cit...   \n",
       "1  blackassign0002  https://insights.blackcoffer.com/rising-it-cit...   \n",
       "2  blackassign0003  https://insights.blackcoffer.com/internet-dema...   \n",
       "3  blackassign0004  https://insights.blackcoffer.com/rise-of-cyber...   \n",
       "4  blackassign0005  https://insights.blackcoffer.com/ott-platform-...   \n",
       "\n",
       "                                             Heading  \\\n",
       "0  Rising IT cities and its impact on the economy...   \n",
       "1  Rising IT Cities and Their Impact on the Econo...   \n",
       "2  Internet Demand’s Evolution, Communication Imp...   \n",
       "3  Rise of Cybercrime and its Effect in upcoming ...   \n",
       "4  OTT platform and its impact on the entertainme...   \n",
       "\n",
       "                                                Text  \n",
       "0  We have seen a huge development and dependence...  \n",
       "1  Throughout history, from the industrial revolu...  \n",
       "2  Introduction In the span of just a few decades...  \n",
       "3  The way we live, work, and communicate has unq...  \n",
       "4  The year 2040 is poised to witness a continued...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iterating over DataFrame's rows to assign headings and text\n",
    "for index, row in df.iterrows():\n",
    "    url = row['URL']\n",
    "    heading, text = get_article_text_and_heading(url)\n",
    "    # Storing the results in the dataframe\n",
    "    df.at[index, 'Heading'] = heading\n",
    "    df.at[index, 'Text'] = text\n",
    "    \n",
    "df.head() # Dataframe now contains individual URL's Article Heading and Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b99a81-b40f-44b1-ad9a-692f6ca1d00f",
   "metadata": {},
   "source": [
    "##### We can see the DataFrame now contains each URLs Article Heading and Text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6aaa105-f5ca-4fa4-961a-79d86f81ac56",
   "metadata": {},
   "source": [
    "Now, in the problem statement, we have been given a list of Stopwords which are to be excluded from the text. Hence, we first bring the list of StopWords into python and then use it to clean our “Text” column in the DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1514bcae-fc75-4fab-b0ef-dead729e62a9",
   "metadata": {},
   "source": [
    "For this, we have defined two functions: read_and_tokenize_files and remove_stopwords.\n",
    "read_and_tokenize_files:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213702ad-045c-4cf9-9e3e-534d8974ef6f",
   "metadata": {},
   "source": [
    "1. **read_and_tokenize_files** :\n",
    "Here, for a given “path”, we open all the files in the path one by one and read the content of \n",
    "the same. Using nltk, we tokenize the text into words and then filter them so that they only \n",
    "contain characters which are further made lowercase. The content of all the files in the path are compiled into a list which is returned to the user (effectively making a list of words from all the files present in the provided path)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e2460d4-20be-43cd-954f-abf6e583b28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function for reading files and tokenizing \n",
    "def read_and_tokenize_files(path):\n",
    "    all_files_in_path = os.listdir(path)\n",
    "    all_tokens = []\n",
    "    for file in all_files_in_path:\n",
    "        with open(f\"{path}/{file}\",\"r\") as f:\n",
    "            file_text = f.read()\n",
    "            tokens = word_tokenize(file_text)\n",
    "            only_words = [token for token in tokens if token.isalpha()] # filtering only words\n",
    "            all_tokens.extend(only_words)\n",
    "            all_tokens= [token.lower() for token in all_tokens] \n",
    "    return all_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdf2994-f5dc-41f9-933c-06a39910c1d4",
   "metadata": {},
   "source": [
    "2. **remove_stopwords** :\n",
    "We use this function to filter out the stopwords from the provided text using the mentioned \n",
    "list of stop tokens. We tokenize the provided text into words and subsequently filter by \n",
    "removing stopwords using list comprehension. We return the obtained list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79c13681-ab61-4587-bf1c-318b6faf4930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize and remove stopwords from text\n",
    "def remove_stopwords(text,stop_tokens):\n",
    "    tokens = word_tokenize(text)\n",
    "    article_words = [token for token in tokens if token.isalpha()]\n",
    "    article_words_wstpw = [word for word in article_words if word.lower() not in stop_tokens] # filtering out StopWords\n",
    "    return article_words_wstpw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02ff54f-59bb-445c-9b0b-f7d8423ad34c",
   "metadata": {},
   "source": [
    "Now, firstly, using the provided StopWords file, we read and tokenize the same into a list \n",
    "which is further passed along with each article text in the DataFrame one by one effectively \n",
    "filtering out the stopwords. We then save the clean tokenized article words list into another \n",
    "column. This is done using the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8189b6dd-2516-44a1-9712-8958135bf30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading all the stopwords files and compiling into one list for easier checking\n",
    "path = \"./assign_ques_details/StopWords/\"\n",
    "all_stop_tokens = read_and_tokenize_files(path)\n",
    "df[\"Text_without_stopwords\"]=df[\"Text\"].apply(remove_stopwords, args=(all_stop_tokens,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2893438-2077-4bcb-8398-d91d825ab4e2",
   "metadata": {},
   "source": [
    "We can see that we have a column containing the clean text now : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ee59c6b-4b12-4aff-bf9e-3f66cac98608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [huge, development, dependence, people, techno...\n",
       "1     [history, industrial, revolution, century, dev...\n",
       "2     [Introduction, span, decades, internet, underg...\n",
       "3     [live, work, communicate, unquestionably, chan...\n",
       "4     [poised, witness, continued, revolution, world...\n",
       "                            ...                        \n",
       "95    [Epidemics, general, direct, indirect, costs, ...\n",
       "96    [COVID, bought, world, knees, businesses, shut...\n",
       "97    [Handicrafts, making, crafts, called, handicra...\n",
       "98    [pay, make, online, payment, lockdown, lockdow...\n",
       "99    [business, prevent, transmission, financial, c...\n",
       "Name: Text_without_stopwords, Length: 100, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Text_without_stopwords\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30224ce-2b3e-44bf-88c8-6128d25051e7",
   "metadata": {},
   "source": [
    "We have 2 text files (from the MasterDictionary file provided) containing positive and negative tokens. We read and tokenize the same into 2 lists for further use in the code snippet below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8da2d251-b2e8-4c23-a1e6-e6a55e73f459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating separate lists for positive and negative tokens from Master Dictionary provided\n",
    "m_dict_path = \"./assign_ques_details/MasterDictionary/\"\n",
    "files_in_path = os.listdir(m_dict_path)\n",
    "p_tokens = [] \n",
    "n_tokens = []\n",
    "\n",
    "for file in files_in_path:\n",
    "    with open(f\"{m_dict_path}{file}\",\"r\") as f:\n",
    "        file_text = f.read()\n",
    "        tokens = word_tokenize(file_text)\n",
    "        if file==\"positive-words.txt\":\n",
    "            p_tokens.extend(tokens)\n",
    "        else:\n",
    "            n_tokens.extend(tokens)\n",
    "        p_tokens= [token.lower() for token in p_tokens]\n",
    "        n_tokens= [token.lower() for token in n_tokens]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bb4928-0ee6-4923-a1b5-c2f9145042eb",
   "metadata": {},
   "source": [
    "*Now, as mentioned in the Objective file, we need to calculate various output parameters as defined in the \"Text Analysis.docx\" file.*\n",
    "\n",
    "For calculating the positive and negative scores, we define another function called \n",
    "score_calculator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e290677c-6e20-42f3-9061-0b8aecf20834",
   "metadata": {},
   "outputs": [],
   "source": [
    "## General function for calculating positive and negative scores\n",
    "def score_calculator(text, token_list):\n",
    "    score = 0\n",
    "    for word in text:\n",
    "        if word.lower() in token_list:\n",
    "            score+=1\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a34fee5-8ca9-4b7a-84b5-4ee650bb6a35",
   "metadata": {},
   "source": [
    "We calculate the score on the basis of each word’s presence in the provided token list and \n",
    "then return the same. \n",
    "\n",
    "Using this function, we calculate the positive and negative scores in the code below :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "279f2eee-9b7c-4f77-9025-09098f74859e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating all scores by iterating over clean text columns\n",
    "df[\"POSITIVE SCORE\"] = df[\"Text_without_stopwords\"].apply(score_calculator, args=(p_tokens,))\n",
    "df[\"NEGATIVE SCORE\"] = df[\"Text_without_stopwords\"].apply(score_calculator, args=(n_tokens,))\n",
    "df[\"POLARITY SCORE\"] = (df[\"POSITIVE SCORE\"] - df[\"NEGATIVE SCORE\"])/ ((df[\"POSITIVE SCORE\"] + df[\"NEGATIVE SCORE\"]) + 0.000001)\n",
    "df[\"SUBJECTIVITY SCORE\"] = (df[\"POSITIVE SCORE\"] + df[\"NEGATIVE SCORE\"])/ (len(df[\"Text_without_stopwords\"]) + 0.000001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78f39da-15df-4de7-bcf4-31dc4a631185",
   "metadata": {},
   "source": [
    "We calculate the syllable count using the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43ae9901-68b9-45a6-ab38-01b0aa808af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count syllables in a word\n",
    "def syllable_count(word):\n",
    "    vowels = 'aeiou'\n",
    "    syllable_counter = 0\n",
    "    for ch in word.lower():\n",
    "        if ch in vowels:\n",
    "            syllable_counter+=1\n",
    "    if word.endswith(\"es\") or word.endswith(\"ed\"): # handling required exceptions\n",
    "        syllable_counter-=1\n",
    "    return syllable_counter\n",
    "\n",
    "    ## Alternate approach using CMU dictionary (not utilised since too computationally expensive)\n",
    "    # d = nltk.corpus.cmudict.dict() # Loading CMU Dictionary\n",
    "    # if word.lower() not in d:\n",
    "    #     return 0 # If word is not found in the dictionary, return 0\n",
    "    # pronunciations = d[word.lower()]\n",
    "    # syllable_counter=0\n",
    "    # # Iterating over all pronunciations of the word and increasing count when last character of an item \n",
    "    # # in the pronounciation list is a digit (indicating a syllable)\n",
    "    # for prn in pronunciations:\n",
    "    #     for itm in prn:\n",
    "    #         if itm[-1].isdigit():\n",
    "    #             syllable_counter+=1\n",
    "    # return syllable_counter\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a9c97e-b0f2-4aac-8293-38615f0c313e",
   "metadata": {},
   "source": [
    "Next, for calculating the multiple output parameters, we have defined a function called \n",
    "‘aor_scores’:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a700bd30-1481-4510-a00e-c381ad01fcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating multiple scores/output labels\n",
    "def aor_scores(text):\n",
    "    complex_word_count = 0\n",
    "    total_word_length_sum = 0\n",
    "    total_syllables = 0\n",
    "    \n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    all_words = word_tokenize(text)\n",
    "    only_words = [word for word in all_words if word.isalpha()]\n",
    "    \n",
    "    avg_sentence_length = len(only_words) / len(sentences) \n",
    "    \n",
    "    for word in only_words:\n",
    "        total_word_length_sum+= len(word) # calculating sum of number of letters in all words\n",
    "        total_syllables+=syllable_count(word)\n",
    "        if syllable_count(word)>=2:\n",
    "            complex_word_count+=1\n",
    "\n",
    "    syllable_count_per_word = total_syllables / len(only_words)\n",
    "    avg_word_length = total_word_length_sum / len(only_words) \n",
    "    complex_words_percentage = ( complex_word_count / len(only_words) )*100\n",
    "    fog_index = 0.4 * (avg_sentence_length + complex_words_percentage)\n",
    "    \n",
    "    return avg_sentence_length, complex_word_count, len(only_words), syllable_count_per_word, complex_words_percentage, fog_index, avg_word_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb914dcf-b751-4bba-9463-44371713093e",
   "metadata": {},
   "source": [
    "We use this function to calculate various output parameters: Firstly, we tokenize the provided article text into sentences as well as words. Using this, we calculate multiple output parameters.\n",
    "\n",
    "*Note: For output parameter description, kindly refer to \"Text Analysis.docx\" file.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6666fa5f-6cde-49f6-b012-20ee905cc013",
   "metadata": {},
   "source": [
    "We return the same and assign them to new columns in our pandas DataFrame :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53f94bd4-7aa4-4b51-9d11-30459812dda2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculating parameter values\n",
    "df[[\"AVG SENTENCE LENGTH\",\"COMPLEX WORD COUNT\",\"WORD COUNT\", \"SYLLABLE COUNT PER WORD\",\"PERCENTAGE OF COMPLEX WORDS\",\"FOG INDEX\",\n",
    "    \"AVG WORD LENGTH\"]] = df[\"Text\"].apply(aor_scores).apply(pd.Series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50c1c93-7dc0-4d1b-82f0-5e0fb01aff77",
   "metadata": {},
   "source": [
    "We calculate the number of pronouns using count_personal_pronouns function and save them into a new column: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "72379df6-b3a4-4917-9aab-873e56ccd127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for counting pronouns using regex\n",
    "def count_personal_pronouns(text):\n",
    "    pattern = r'\\b(I|WE|MY|OURS|We|My|Ours|Us|we|my|ours|us|i)\\b' # US case handling by mentioning all possible cases explicitly\n",
    "    matches = re.findall(pattern, text)\n",
    "    count = len(matches)\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d8271424-4164-4668-b17e-34fcc539814a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"PERSONAL PRONOUNS\"] = df[\"Text\"].apply(count_personal_pronouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb57d23-fa09-4e4d-8fa8-eccf0da239a2",
   "metadata": {},
   "source": [
    "Using the given range, we can also check for any outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e42fae66-3f46-4f94-8287-9d29ae4653e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking Outliers for Subjectivity and Polarity Scores\n",
    "# df[df[\"SUBJECTIVITY SCORE\"]>1]\n",
    "# df[(df[\"POLARITY SCORE\"]>1)|(df[\"POLARITY SCORE\"]<-1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef82fb83-be30-41a3-8a90-c1999ff85a76",
   "metadata": {},
   "source": [
    "**Now, we have a DataFrame with all the required columns.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d218417e-f23f-4278-994b-d56c736cbe72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>Heading</th>\n",
       "      <th>Text</th>\n",
       "      <th>Text_without_stopwords</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE COUNT PER WORD</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blackassign0001</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "      <td>Rising IT cities and its impact on the economy...</td>\n",
       "      <td>We have seen a huge development and dependence...</td>\n",
       "      <td>[huge, development, dependence, people, techno...</td>\n",
       "      <td>26</td>\n",
       "      <td>6</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.32</td>\n",
       "      <td>15.207792</td>\n",
       "      <td>558.0</td>\n",
       "      <td>1171.0</td>\n",
       "      <td>1.738685</td>\n",
       "      <td>47.651580</td>\n",
       "      <td>25.143749</td>\n",
       "      <td>4.574722</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blackassign0002</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "      <td>Rising IT Cities and Their Impact on the Econo...</td>\n",
       "      <td>Throughout history, from the industrial revolu...</td>\n",
       "      <td>[history, industrial, revolution, century, dev...</td>\n",
       "      <td>51</td>\n",
       "      <td>29</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>0.80</td>\n",
       "      <td>18.350649</td>\n",
       "      <td>813.0</td>\n",
       "      <td>1413.0</td>\n",
       "      <td>2.058740</td>\n",
       "      <td>57.537155</td>\n",
       "      <td>30.355122</td>\n",
       "      <td>5.454352</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blackassign0003</td>\n",
       "      <td>https://insights.blackcoffer.com/internet-dema...</td>\n",
       "      <td>Internet Demand’s Evolution, Communication Imp...</td>\n",
       "      <td>Introduction In the span of just a few decades...</td>\n",
       "      <td>[Introduction, span, decades, internet, underg...</td>\n",
       "      <td>36</td>\n",
       "      <td>23</td>\n",
       "      <td>0.220339</td>\n",
       "      <td>0.59</td>\n",
       "      <td>18.446429</td>\n",
       "      <td>614.0</td>\n",
       "      <td>1033.0</td>\n",
       "      <td>2.240077</td>\n",
       "      <td>59.438529</td>\n",
       "      <td>31.153983</td>\n",
       "      <td>6.042594</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blackassign0004</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-cyber...</td>\n",
       "      <td>Rise of Cybercrime and its Effect in upcoming ...</td>\n",
       "      <td>The way we live, work, and communicate has unq...</td>\n",
       "      <td>[live, work, communicate, unquestionably, chan...</td>\n",
       "      <td>35</td>\n",
       "      <td>74</td>\n",
       "      <td>-0.357798</td>\n",
       "      <td>1.09</td>\n",
       "      <td>20.137255</td>\n",
       "      <td>600.0</td>\n",
       "      <td>1027.0</td>\n",
       "      <td>2.179163</td>\n",
       "      <td>58.422590</td>\n",
       "      <td>31.423938</td>\n",
       "      <td>5.937683</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blackassign0005</td>\n",
       "      <td>https://insights.blackcoffer.com/ott-platform-...</td>\n",
       "      <td>OTT platform and its impact on the entertainme...</td>\n",
       "      <td>The year 2040 is poised to witness a continued...</td>\n",
       "      <td>[poised, witness, continued, revolution, world...</td>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.26</td>\n",
       "      <td>17.289474</td>\n",
       "      <td>360.0</td>\n",
       "      <td>657.0</td>\n",
       "      <td>1.981735</td>\n",
       "      <td>54.794521</td>\n",
       "      <td>28.833598</td>\n",
       "      <td>5.394216</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            URL_ID                                                URL  \\\n",
       "0  blackassign0001  https://insights.blackcoffer.com/rising-it-cit...   \n",
       "1  blackassign0002  https://insights.blackcoffer.com/rising-it-cit...   \n",
       "2  blackassign0003  https://insights.blackcoffer.com/internet-dema...   \n",
       "3  blackassign0004  https://insights.blackcoffer.com/rise-of-cyber...   \n",
       "4  blackassign0005  https://insights.blackcoffer.com/ott-platform-...   \n",
       "\n",
       "                                             Heading  \\\n",
       "0  Rising IT cities and its impact on the economy...   \n",
       "1  Rising IT Cities and Their Impact on the Econo...   \n",
       "2  Internet Demand’s Evolution, Communication Imp...   \n",
       "3  Rise of Cybercrime and its Effect in upcoming ...   \n",
       "4  OTT platform and its impact on the entertainme...   \n",
       "\n",
       "                                                Text  \\\n",
       "0  We have seen a huge development and dependence...   \n",
       "1  Throughout history, from the industrial revolu...   \n",
       "2  Introduction In the span of just a few decades...   \n",
       "3  The way we live, work, and communicate has unq...   \n",
       "4  The year 2040 is poised to witness a continued...   \n",
       "\n",
       "                              Text_without_stopwords  POSITIVE SCORE  \\\n",
       "0  [huge, development, dependence, people, techno...              26   \n",
       "1  [history, industrial, revolution, century, dev...              51   \n",
       "2  [Introduction, span, decades, internet, underg...              36   \n",
       "3  [live, work, communicate, unquestionably, chan...              35   \n",
       "4  [poised, witness, continued, revolution, world...              18   \n",
       "\n",
       "   NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  AVG SENTENCE LENGTH  \\\n",
       "0               6        0.625000                0.32            15.207792   \n",
       "1              29        0.275000                0.80            18.350649   \n",
       "2              23        0.220339                0.59            18.446429   \n",
       "3              74       -0.357798                1.09            20.137255   \n",
       "4               8        0.384615                0.26            17.289474   \n",
       "\n",
       "   COMPLEX WORD COUNT  WORD COUNT  SYLLABLE COUNT PER WORD  \\\n",
       "0               558.0      1171.0                 1.738685   \n",
       "1               813.0      1413.0                 2.058740   \n",
       "2               614.0      1033.0                 2.240077   \n",
       "3               600.0      1027.0                 2.179163   \n",
       "4               360.0       657.0                 1.981735   \n",
       "\n",
       "   PERCENTAGE OF COMPLEX WORDS  FOG INDEX  AVG WORD LENGTH  PERSONAL PRONOUNS  \n",
       "0                    47.651580  25.143749         4.574722                 12  \n",
       "1                    57.537155  30.355122         5.454352                  4  \n",
       "2                    59.438529  31.153983         6.042594                 13  \n",
       "3                    58.422590  31.423938         5.937683                  5  \n",
       "4                    54.794521  28.833598         5.394216                  6  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39cdfac-c15b-41fe-bb27-c077e4be1c79",
   "metadata": {},
   "source": [
    "We remove the columns which are not required and re-order the same. We then save the DataFrame to an output file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4bc256da-5685-4eb7-b743-f971c3b22dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Final output in another dataframe\n",
    "df2=df.drop([\"Text\",\"Heading\",\"Text_without_stopwords\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "afdf1f44-7288-4eb3-8a64-757b7a2b86d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reordering columns and saving to a final dataframe\n",
    "df_final = df2.loc[:,[\"URL_ID\", \"URL\",\t\"POSITIVE SCORE\", \"NEGATIVE SCORE\", \"POLARITY SCORE\", \"SUBJECTIVITY SCORE\",\t\"AVG SENTENCE LENGTH\",\t\"PERCENTAGE OF COMPLEX WORDS\",\t\"FOG INDEX\",\t\"COMPLEX WORD COUNT\",\t\"WORD COUNT\",\t\"SYLLABLE COUNT PER WORD\",\t\"PERSONAL PRONOUNS\", \"AVG WORD LENGTH\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3a9e29cc-99b2-4694-8a33-e33190b20283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE COUNT PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blackassign0001</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "      <td>26</td>\n",
       "      <td>6</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.32</td>\n",
       "      <td>15.207792</td>\n",
       "      <td>47.651580</td>\n",
       "      <td>25.143749</td>\n",
       "      <td>558.0</td>\n",
       "      <td>1171.0</td>\n",
       "      <td>1.738685</td>\n",
       "      <td>12</td>\n",
       "      <td>4.574722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blackassign0002</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "      <td>51</td>\n",
       "      <td>29</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>0.80</td>\n",
       "      <td>18.350649</td>\n",
       "      <td>57.537155</td>\n",
       "      <td>30.355122</td>\n",
       "      <td>813.0</td>\n",
       "      <td>1413.0</td>\n",
       "      <td>2.058740</td>\n",
       "      <td>4</td>\n",
       "      <td>5.454352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blackassign0003</td>\n",
       "      <td>https://insights.blackcoffer.com/internet-dema...</td>\n",
       "      <td>36</td>\n",
       "      <td>23</td>\n",
       "      <td>0.220339</td>\n",
       "      <td>0.59</td>\n",
       "      <td>18.446429</td>\n",
       "      <td>59.438529</td>\n",
       "      <td>31.153983</td>\n",
       "      <td>614.0</td>\n",
       "      <td>1033.0</td>\n",
       "      <td>2.240077</td>\n",
       "      <td>13</td>\n",
       "      <td>6.042594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blackassign0004</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-cyber...</td>\n",
       "      <td>35</td>\n",
       "      <td>74</td>\n",
       "      <td>-0.357798</td>\n",
       "      <td>1.09</td>\n",
       "      <td>20.137255</td>\n",
       "      <td>58.422590</td>\n",
       "      <td>31.423938</td>\n",
       "      <td>600.0</td>\n",
       "      <td>1027.0</td>\n",
       "      <td>2.179163</td>\n",
       "      <td>5</td>\n",
       "      <td>5.937683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blackassign0005</td>\n",
       "      <td>https://insights.blackcoffer.com/ott-platform-...</td>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.26</td>\n",
       "      <td>17.289474</td>\n",
       "      <td>54.794521</td>\n",
       "      <td>28.833598</td>\n",
       "      <td>360.0</td>\n",
       "      <td>657.0</td>\n",
       "      <td>1.981735</td>\n",
       "      <td>6</td>\n",
       "      <td>5.394216</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            URL_ID                                                URL  \\\n",
       "0  blackassign0001  https://insights.blackcoffer.com/rising-it-cit...   \n",
       "1  blackassign0002  https://insights.blackcoffer.com/rising-it-cit...   \n",
       "2  blackassign0003  https://insights.blackcoffer.com/internet-dema...   \n",
       "3  blackassign0004  https://insights.blackcoffer.com/rise-of-cyber...   \n",
       "4  blackassign0005  https://insights.blackcoffer.com/ott-platform-...   \n",
       "\n",
       "   POSITIVE SCORE  NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  \\\n",
       "0              26               6        0.625000                0.32   \n",
       "1              51              29        0.275000                0.80   \n",
       "2              36              23        0.220339                0.59   \n",
       "3              35              74       -0.357798                1.09   \n",
       "4              18               8        0.384615                0.26   \n",
       "\n",
       "   AVG SENTENCE LENGTH  PERCENTAGE OF COMPLEX WORDS  FOG INDEX  \\\n",
       "0            15.207792                    47.651580  25.143749   \n",
       "1            18.350649                    57.537155  30.355122   \n",
       "2            18.446429                    59.438529  31.153983   \n",
       "3            20.137255                    58.422590  31.423938   \n",
       "4            17.289474                    54.794521  28.833598   \n",
       "\n",
       "   COMPLEX WORD COUNT  WORD COUNT  SYLLABLE COUNT PER WORD  PERSONAL PRONOUNS  \\\n",
       "0               558.0      1171.0                 1.738685                 12   \n",
       "1               813.0      1413.0                 2.058740                  4   \n",
       "2               614.0      1033.0                 2.240077                 13   \n",
       "3               600.0      1027.0                 2.179163                  5   \n",
       "4               360.0       657.0                 1.981735                  6   \n",
       "\n",
       "   AVG WORD LENGTH  \n",
       "0         4.574722  \n",
       "1         5.454352  \n",
       "2         6.042594  \n",
       "3         5.937683  \n",
       "4         5.394216  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3c14c2-2ea5-4e6b-b959-0362cda6dd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to excel file if required\n",
    "df_final.to_excel(\"final_output_vishrut.xlsx\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6eb04c-d6d4-4b66-8ab4-2fe1e75f87f0",
   "metadata": {},
   "source": [
    "#### This completes the required task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04843e4a-7632-4b76-a1a9-44bfe30536fb",
   "metadata": {},
   "source": [
    "##### *Additional Help*\n",
    "**<u>How to run the file</u>:** \n",
    "\n",
    "The file can be run in any IDE directly but special care has to be taken to edit the paths \n",
    "mentioned in the code for the provided files (positive-words, negative-words, input.xlsx \n",
    "etc.).  \n",
    "Please note, the code block containing the URL scrapping may take some time to execute \n",
    "based on the system capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4166228d-f9d9-4130-bb1d-c1bb2b1902ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
